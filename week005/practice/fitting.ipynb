{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fittingについてもう少し\n",
    "フィッティング(近似手法/推定など)は\"あるサンプル\"に対して数式当てはめを行う手法のこと。\n",
    "<br>そのため、機械学習では、回帰分析(あるサンプルから数式を求め、未知データを予測する)や、\n",
    "<br>Deep　Learning(あるサンプルからの特徴量(=説明変数)探しの際の誤差の最小化)などに使われる。\n",
    "\n",
    "大学時代は近似とか推定という名前をよく使っていた気がする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主なFittingの手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + 最尤推定\n",
    "     + 最小二乗法\n",
    "         + Gauss・Newton法 (非線形最小二乗法の一つ)\n",
    " + 勾配法\n",
    "     + 準ニュートン法\n",
    "     + 共役勾配法\n",
    "     + 最急降下法\n",
    "     + 確率勾配降下法\n",
    "     + ミニバッチ確率的勾配降下法\n",
    " + コベル法 \n",
    " \n",
    " 徐々に触れていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最尤推定 (最尤法)\n",
    "確率分布関数(正規分布とか)がわかっている時、尤度(尤もらしさ)が最も高くなる値を探すことでパラメータ推定を行う手法。\n",
    "\n",
    "確率分布関数\n",
    "$$f_D(x_1,...,x_n | \\theta)$$\n",
    "尤度関数 (尤度を求める関数)\n",
    "$$L(\\theta ) = f_D(x_1,....,x_n|\\theta)$$\n",
    "\n",
    "このLが最大になるようなθを、最尤推定量 (maximum likelihood estimator)という。<br>\n",
    "また、パラメータの推定値となるように扱う。<br>\n",
    "この最尤推定量は、よく以下の式の解として扱う。\n",
    "$$ \\frac{\\partial }{\\partial \\theta} log L(\\theta) = 0$$\n",
    "\n",
    "確率分布が行列になる場合は、コレスキー分解orヘッセ行列法による逆行列計算をすることになる。\n",
    "逆行列計算が発生する場合、計算コストが高い。\n",
    "$$(O(n^3) 　　n: 行数)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 最小二乗法\n",
    "最小二乗法とは、最尤推定の一つ。最も多く使われる推定法の一つ、前回・前々回使用した回帰でも使われる。\n",
    "\n",
    "モデル関数を $$f(x)$$とする時、\n",
    "$$\\Sigma^{n}_{i=1} \\{y_i - f(x)\\}^2$$\n",
    "が最小となるようにf(x)を求めること。\n",
    "\n",
    "例えば、回帰モデルを$$y=ax+b$$とする時、\n",
    "$$a=\\frac{\\Sigma^{n}_{i=1}(x_i - x_{mean})(y_i - y_{mean})}{\\Sigma^n_{n=1}(x_i - x_{mean})^2}$$\n",
    "$$b = y_{mean} - ax_{mean}$$\n",
    "と表せ、解を解くことでパラメータ推定を行う手法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配降下法\n",
    "パラメータ推定の際に関数の勾配(傾き)から解を求めるアルゴリズムの総称。\n",
    "(極値により近い値を探索することで、パラメータを推定する。)\n",
    "\n",
    " + 局所的な最小値に止まり安く、大局的な最小値を求めるのが困難。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  最急降下法\n",
    "n次ベクトル\n",
    "$$ x = (x_1,..., x_n) $$\n",
    "からなる関数\n",
    "f(x)\n",
    "の極小値を反復することで求める手法。\n",
    "$$ x^{(k+1z)} = x^{(k)} - \\alpha \\frac{\\partial{}}{\\partial{x^{(x)}}} f(x^{(k)})  $$\n",
    "\n",
    " + 傾き(一階微分)のみしか見ないので計算が早い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 確率的勾配降下法\n",
    "機械学習でよく使用される手法。\n",
    "勾配法の局所解を求めてしまうことへの対策を行った手法。\n",
    "##### ミニバッチ確率的勾配降下法\n",
    "確率的勾配降下法をオンライン学習に適用した手法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
